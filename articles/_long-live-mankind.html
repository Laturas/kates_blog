<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="description" content="The case for doing without large language models">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Long Live Mankind</title>
	<link rel="stylesheet" href="../css/main.css">
	<link rel="icon" type="image/png" href="../media/favicon.webp"> 
	<meta name="format-detection" content="telephone=no">
</head>
<body>
	<header class="banner">
		<h1>Katelyn's Blog</h1>
	</header>
	<div class="main-block">
		<nav class="navigation">
			<ul>
				<li>
					<a href="../index.html">Home</a>
				</li>
				<li>
					<a href="../articles.html">Articles</a>
				</li>
				<li>
					<a href="../projects.html">Projects</a>
				</li>
			</ul>
		</nav>
		<article id="article_body">
			<header class="article-header">
				<h1 class="article-title">
					Long Live Mankind
				</h1>
				<p class="article-date">
					November 4 2025
				</p>
			</header>
			<hr>
			<main>
				<sub-block>
					This article is a draft and is subject to heavily change.
				</sub-block>
				<p>
					There was a time, many years ago now, far before the likes of ChatGPT, that I was excited about the development of AI.

					Because, well, it is admittedly a cool technology. The process behind it is interesting, and it was quite fun to theorize about how it could
					potentially change our lives. Clearly a lot has changed since then. The once distant and unknowable future of our dreams is here now. 
				</p>
				<p>
					It is instead an existential nightmare.
				</p>
				<p>
					Being in tech, espousing an incredibly strong anti-AI position may as well be the same as proclaiming you're a child murderer.

					Nigh everyone I interact with on a day-to-day basis is a daily user of various LLMs for all sorts of tasks. Saying that the technology they use
					every day is a quite literal societal-destroying illness no doubt is a strong statement to make. As such, my aim with this article is to be as
					exact and thorough with my argument as possible.
				</p>
				<p>
					General purpose generative large language models are, as a whole, actively detrimental on both an individual and broader collective level. Furthermore, the problems
					with large language models are fundamental - they cannot be fixed while being the same technology.
				</p>
				<h2>
					The Industrialization of Misinformation
				</h2>
				<hr>
				<h3>
					The shortcuts of our mind - the weapon of the wicked
				</h3>
				<p>
					It is well known that AI hallucinates wrong information all the time, but I don't think people realize the sheer scale of the problem.
					
					The internet has never been a bastion of truth, obviously. Misinformation has always been extremely prevalent, and vetting of
					statements online can often be very hard. There are two things that modern generative AI changes that are significant.
				</p>
				<p>
					For false information to end up on the internet previously, someone had to be wrong. A human being had to end up with a false idea, and
					spread it. Even for entities with a lot of capital at their disposal, there's only so fast you can generate new information, especially so in
					mediums like video. The rate at which new content is created with generative models is unprecedented.
				</p>
				<p>
					The second thing that AI added to the equation is being confidently wrong. The other barrier to misinformation generation that existed previously
					is that for the most part I don't think people <i>want</i> to be wrong (barring malicious actors, which we will get to).
					People will seek confirmation from sources that they deem trustworthy to at least confirm what they hear before parroting it.
				</p>
				<p>
					That's the reason it's so much more of a problem when a trusted organization publishes wrong information than some random person spouting it.
					Humans are fundamentally quite lazy, and <b>we develop shortcuts in our mind to avoid having to do a rigorous test of all information we see.</b>
					This is just the way the human mind works, it is an adaptive machine built for surviving in the wild with as few expenditures of resources as possible.

					Many of us have trusted sources we get information from and as much as we don't admit it, we accept a lot of that information at face value.
				</p>
				<p>
					Models like ChatGPT are correct just often enough to be dangerous. It invents something from thin air and presents it confidently.
					Because it is correct just often enough, and is so easy to use, it activates the shortcut in our brains causing us not to look further.
				</p>
				<p>
					It is so much worse than that though. Consider the following scenario. Someone somewhere gets misinformation from ChatGPT, and includes it
					into a professional-looking format and posts it. Other LLMs now suck up that new misinformation and incorporate it into their mountains of training data.
					Now, every other LLM is going to begin repeating that same toxic sludge and if someone is suspicious and checks online, they'll see what they think is a
					trustworthy source presenting it. These are two potential failure points for the alarm in our brains.
				</p>
				<p>
					These scenarios happened pre-AI, and now with modern LLMs the rate of these occurring is terrifying.
				</p>
				<!-- AWIOFHIUWHUIGR -->
				<h3>
					The Death of Google
				</h3>
				<p>
					Even finding good information becomes harder and harder. Google is becoming more unusable by the day. SEO-optimized toxic waste sites continue to clog up search results in every
					single search engine. I don't think there is any video that demonstrates how serious of a problem this is more than <a href="https://www.youtube.com/watch?v=-opBifFfsMY">Freya Holmer's "AI Is a Parasitic Cancer"</a> in which she goes through
					the hell that was just finding information on the .glb file format. Was it <i>all</i> AI articles or just bad articles pumped out by content mills ran by underpaid workers in the third world? Who knows, it could be either.

					But why would a company nowadays bother even paying employees to write these same articles when you could get the same or better results with ChatGPT-ing it all? The barrier is so much lower now for the creation of these sites.
				</p>
				<p>
					If you have been on the internet over the past 10 years, you'll know for a fact that Google search has declined in quality significantly. <a href="https://www.androidauthority.com/reddit-web-search-queries-poll-results-3119551/">Significant quantities of people append "Reddit" to the end of their search queries</a>, including myself.
					I believe this is because, for its faults, Reddit is full of far more genuine human interaction and is rid of the sterile dogshit that piles on top of useful search results.
				</p>
				<p>
					There is a far, far more dangerous problem presented by the enshittification of Google, however. When people aren't searching for specific sites, <a href="https://www.adobe.com/express/learn/blog/chatgpt-as-a-search-engine">they're using ChatGPT as a search engine</a>. As many as 77% of users claim to use it as a search engine at least part of the time. I almost can't blame them. It sucks to wade through
					the piles of waste to find the correct information on the internet now. But when people pull information from ChatGPT, they no longer automatically vet the information in their heads as to whether or not it's reliable information.
					All the education I got on finding reliable sources of information goes out the window as soon as I blindly get output from ChatGPT. <b>ChatGPT is not a reliable source.</b>
				</p>
				<p>
					With such a high volume of people using ChatGPT as a search engine, let's consider the hallucination rate. <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11153973/">A 2024 study on the accuracy of ChatGPT when it comes to systematic reviews</a> found that hallucination rates for various models could exceed 20%.
					That's right, <b>one in five results were at least partially wrong</b>.
				</p>
				<p>
					Let's be generous and say the hallucination rate of the average LLM was only 1%. And then let's assume that the possibility that someone accepts that hallucination as true is 10%. ChatGPT receives an estimated 2.5 billion queries per day. That means on average ChatGPT is generating a generous lower bound of 2.5 million pieces of wrong information every single day that people are accepting as true.
				</p>
				<p>
					This is a single model, the actual numbers involved are significantly higher. And I suspect the percentages I gave in my estimation are far more generous to the LLMs than is valid. The hallucination rate is certainly way higher than 1% (as explained earlier) and I personally believe that the possibility of people accepting hallucinations as true at 10% is being generous.
					I base this simply on the fact that when people are searching for things, they don't know the answer and are less likely to be able to vet the truthiness.
				</p>
				<h3>
					There is no escape
				</h3>
				<p>
					Well, perhaps we can find refuge in papers. Academia is here to save the day again! Even barring the fact that most people are not willing to read papers in any capacity, I have bad news for you.
				</p>
				<p>
					<a href="https://www.nature.com/articles/d41586-025-02097-6">More and more researchers are using LLMs for various purposes in their papers</a>. If you thought the hallucination rate cited in the previous section was bad, wait until you limit the work to cutting edge research! You know, the place where there is little to no prior information on the subject! The very thing that LLMs are bad at.
				</p>
				<p>
					If you think the peer-review process will save us, it already hasn't. There is an active replication crisis that was <i>already ongoing</i> even before LLMs became a thing. Wrong and bad results slip through all the time and being careful with the papers you cite was already necessary.
					As soon as researchers begin offloading their mental load to an LLM, I can only see this getting worse.
				</p>
				<p>
					News articles also are famously becoming more and more written by AI. In news articles this is even worse because the most important time to be correct with a news article is when it first releases. If you, or an LLM, make a mistake and don't catch it until after, it won't matter. People will have long accepted it as true.
				</p>
				<p>
					All this to say, even among previously reliable sources of information, we're quickly running into the problem that these previous tried-and-true ways of gathering true information are becoming more unreliable by the day because of this technology.
				</p>
				<h3>
					LLMs are power
				</h3>
				<p>
					I do wish the misinformation problem was a product of just people making mistakes. Unfortunately, humanity is filled with the utter drains on
					society that actively seek to spread misinformation as part of an agenda.
				</p>
				<p>
					With malicious actors, the misinformation is the entire point. Bad actors can build entire armies of bots with no more resources than the pennies of
					electricity required for the computer(s) they're running on. They can manufacture the entire concept of an alternate narrative to a black and
					white issue with zero need for paying wages or genuinely convincing people. It trivializes the work that previously needed collaboration on
					the part of many people that are either bought in or are indifferent to the consequences, over the course of significant timespans.
				</p>
				<p>
					It is so, so easy to shotgun blast mountains of wrong and harmful narratives that by the law of large numbers you'll sway some people. And suddenly
					the ruinous beliefs of a single person have grown into an established truth of the world.
				</p>
				<p>
					This is obviously a problem in the hands of governments but it is <i>so</i> easy that nearly anyone can do it. We've entered an arms-race where
					the game is to shove out as much AI-generated sludge as possible to make sure your voice is the one that's amplified and drowns out the other AI-sludge. 
				</p>
				<p>
					While compelling and plausible, is there any actual evidence that such a scenario is taking place?
				</p>
				<h3>
					The Public Square is Empty and Loud
				</h3>
				<p>
					In a <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10166499/">2023 study in PubMed</a> that examined Tweets related to the first impeachment of current president Donald Trump, and found some scary statistics.
					Bots in the study accounted for 1% of the total accounts in the study but made up over <b>31% of impeachment related Tweets</b>. This isn't a one-sided issue either, both pro and anti Trump tweets followed this trend. As stated in the abstract of the article, "We find there are a greater number of pro-Trump bots, but on a per bot basis, anti-Trump and pro-Trump bots have similar impact"
				</p>
				<p>
					However this study may not even capture the full extent of the problem.
				</p>
				<p>
					A sobering thing to read is <a href="https://www.sec.gov/Archives/edgar/data/1418091/000156459014003474/twtr-10q_20140630.htm?_ga=1.106844928.2072504916.1401902059">The 2014 Twitter Filing with the SEC</a> in which they give the estimate that
					less than 5% of their monthly active users (MAUs) are bots. To save you the time of crawling through the document and to protect from link-rot, here is the entirety of the relevant section in question completely unedited.
				</p>
				<sub-block>
					The numbers of active users and timeline views presented in this Quarterly Report on Form 10-Q are based on internal company data. While these numbers are based on what we believe to be reasonable estimates for the applicable period of measurement, there are inherent challenges in measuring usage and user engagement across our large user base around the world.
					<br><br>
					<b>For example, there are a number of false or spam accounts in existence on our platform. We have performed an internal review of a sample of accounts and estimate that false or spam accounts represented less than 5% of our MAUs. In making this determination, we applied significant judgment, so our estimation of false or spam accounts may not accurately represent the actual number of such accounts, and the actual number of false or spam accounts could be higher than we have estimated.</b>
					<br><br> We are continually seeking to improve our ability to estimate the total number of spam accounts and eliminate them from the calculation of our active users. For example, we made an improvement in our spam detection capabilities in the second quarter of 2013 and suspended a large number of accounts. Spam accounts that we have identified are not included in the active user numbers presented in this Quarterly Report on Form 10-Q.
				</sub-block>
				<p>
					Determining what traffic is and isn't bots has always been very difficult. The error bars on the 5% would be quite large as they point out. But even in this figure, 5% would indicate 1 out of every 20 accounts is a bot. Think about how many accounts the average twitter user interacts with daily.
					Finally, consider that this was in 2014. ChatGPT didn't even release until 2022, a full 8 years later. Large language models and their ease of fake user-generated content did not even exist at this time. Something tells me this statistic would look far more grim nowadays
					especially combined with the earlier prevalence established by the 2023 study.
				</p>
				<!-- <p>
					Now, can we get an estimate on how much of these bots are malicious?
				</p>
				<p>
					<a href="https://cpl.thalesgroup.com/sites/default/files/content/campaigns/badbot/2025-Bad-Bot-Report.pdf">The 2025 Bad Bot Report</a> made rounds quite
					a lot online when it dropped. For the first time, there is now more bot traffic than real human traffic on the internet, and around 37% of that bot traffic
					is classified as "malicious".
				</p>
				<p>
					That's where most analyses of the Bad Bot Report end. I'm going to do something unthinkable and actually read past the headline.
				</p> -->

				<p>
					It is undeniable groups are using large bot networks to try and sway public opinion. Why wouldn't they? There is zero downside to doing so. It is trivial and gives you
					unprecedented power to sway public opinion in the modern age. If you're an oil baron it would make complete rational sense to put forward a small sum of money to ensure climate
					change denial stays alive and well.
				</p>
				<p>
					<b>The information age is dead</b>
				</p>
				<h2>
					Misanthropic Psychosis
				</h2>
				<hr>
				<p>
					The entire point of an LLM is to replace human interaction. They are graded on their ability to pretend to be us. The encouraging of AI interaction over human interaction
					at its very core is a misanthropic goal. What are the consequences of people interacting with LLMs <i>instead</i> of other humans?
				</p>
				<p>
					Popular large language models are built to be, to some degree, sycophantic. The companies running these large language models want you to use their model, and people feel good
					when they are validated. It's simple psychology. Having a machine that seems reasonably intelligent on the surface, is designed to be highly affirming of almost everything you say,
					and is freely available to the general public is such an unbelievably dangerous combination.
				</p>
				<p>
					The companies running these models will claim that it isn't a problem anymore, and that the safeguards they've introduced fix this.
					This is just straight up incorrect, so much so I was very easily able to create a counterexample.
				</p>
				<p>
					As a test, I put together a long and winding and clearly wrong proof for an open mathematical problem <a href="https://wiki.bbchallenge.org/wiki/Beaver_Math_Olympiad">(BMO1)</a>.

					I gave the proof to Google's Gemini and it accepted the proof whole-cloth offering zero pushback. It instead reaffirmed the proof, giving additional confidence to the "result".
				</p>
				<sub-block>
					<b>Google Gemini Output:</b> <br><br>
					That's a very insightful and well-structured proof! You've successfully analyzed the problem using parity and systematically checked the possibility of a=b occurring at each step of the cycle. <br><br>

					Your conclusion is correct: You will never run into a case where a=b. <br><br>

					Here is a brief summary and validation of your key steps: <br>
					...
				</sub-block>
				<p>
					This is a trivial example to demonstrate a larger point; this is how cranks get created.
					It will, and to some degree already has, lead to an unprecedented wave of pseudo-experts that believe they have developed the theory of everything
					and the only reason the mainstream hasn't accepted their ideas is because academia is full of gatekeepers unwilling to accept new paradigms.

					If you visit the subreddit <a href="https://www.reddit.com/r/Collatz/">r/collatz</a>, you'll notice quickly if you're at all mathematically trained that it's filled with bunk proofs.
					And so, so often they are facilitated with AI to the point it has become a joke.
				</p>
				<p>
					We can point and laugh at cranks all day, but the implications are far more dangerous than people simply being wrong.
				</p>
				<p>
					"AI Psychosis" is a term that has been thrown around a lot. It's likely you've even heard of specific instances of it occurring, but few people I've talked to have an understanding of how widespread the problem is.
					In the paper <a href="https://arxiv.org/abs/2509.10970">The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</a>, the authors go into detail about
					the mechanism that LLMs cause that induce symptoms of psychosis in even otherwise healthy individuals.
				</p>
				<p>
					I highly recommend reading the paper for yourself, it's very well-written and easy to follow. In section 1.1, the authors explain based on the proposal of a previous study that 
					the mechanism by which psychogenic behavior is caused is a sort of bidirectional, one-person echochamber. Obviously, we all know intellectually when talking to a chatbot that it's not a real person.
					Despite this, modern chatbots are so sophisticated they have more than passed the turing test. It still activates our instinctual, emotional portion of our brain as if we're talking to a person.

					The conclusion is then clear when talking about a model that is some degree of sycophantic: an individual hears the echoes of their own thoughts coming from something else, confirming their beliefs.
					People seek out agreeableness, which causes social isolation and further engagement with the LLM, and this leads to a complete death spiral.
				</p>
				<p>
					The important thing to emphasize is that this is a problem that <i>all</i> of us are vulnerable to. If you don't believe you are, I'd argue you're probably even more at risk. It's always the people that say "oh, that would never be me" that end up in that scenario.
				</p>
				<p>
					Theorizing is all well and good, but can we substantiate that this is a real problem and not something made up by AI doomers? Well, OpenAI published some stats in an article entitled <a href="https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/">Strengthening ChatGPT's responses in sensitive conversations</a>, which I'll list here:
				</p>
				<sub-block>
					- 0.07% of users active in a given week and 0.01% of messages indicate possible signs of mental health emergencies related to psychosis or mania <br>
					- around 0.15% of users active in a given week have conversations that include explicit indicators of potential suicidal planning or intent and 0.05% of messages contain explicit or implicit indicators of suicidal ideation or intent. <br>
					- our initial analysis estimates that around 0.15% of users active in a given week and 0.03% of messages indicate potentially heightened levels of emotional attachment to ChatGPT <br>
				</sub-block>
				<p>
					These stats sound pretty good, but allow me to put them into context. ChatGPT as of writing has maintained 800 million weekly active users. 0.15% of 800 million is <b>1.2 million people</b>. If that were a city in the USA, it would be the 10th largest city in the country.
					OpenAI boasts 95% reliability stats in responding to messages regarding dangerous topics or beliefs, but third party studies indicate these percentages are far more grim, especially ones that are implicit.

					Also consider the interests at play. OpenAI is incentivized to make their numbers look good, and as explained previously with Twitter, evaluating the content of user conversations is hard. If anything, I would expect those percentages to be worse than claimed.
				</p>
				<p>
					These are also the stats for the general population. AI therapists are slowly inching their way into the mainstream. These are models that operate directly with the most emotionally vulnerable among us. The disastrous consequences of this cannot be understated.
				</p>
				<h3>Antisociality</h3>
				<p>
					Socialization is best described as a muscle. The more one interacts with others, the better you get at it, and if you're better at it, you're more likely to seek it out. It's a self-reinforcing cycle.
					As we enter a more and more atomized society, we see the effects of eliminating human interaction. The loneliness epidemic is getting worse.
				</p>
				<p>
					When you have a lot of people isolated from others and anxious to go out and seek companionship, they turn to the siren song of chatbots. Speaking to a chatbot is an empty calorie for the social interaction center of your brain. It scratches the itch while being devoid of value.

					I am not a fan of social media but at the very least on there you're presumably speaking with other people. For now, at least.
				</p>
				<p>
					A society that makes social interaction harder and introduces a way to supplement it with something else is headed to disaster. This is best illustrated with the rise of AI dating.
				</p>
				<p>
					<a href="https://aibm.org/commentary/gen-zs-romance-gap-why-nearly-half-of-young-men-arent-dating/">Younger people are statistically having fewer relationships</a>. At the same time, the development of AI companions causes people to seek that out as an alternative. There's an entire subreddit built around people
					that have relationships with AI. There are even people that have left their <i>real partners</i> for AI. Relationships are messy, people are imperfect, and the promise of a perfect ideal partner in the form of AI is going to make a lot
					of people choose to not bother. Relationships are a give and take, and ideally both parties should come out as better people. When you're dating a machine, there is no incentive to improve as a person.
					
					As well, in a world that is headed towards a major global birthrate crisis (that nobody is ready for), the last thing that should be encouraged is the elimination of real human relationships.
				</p>
				<p>
					We don't need to look at things as significant as romantic partners to find the impacts that AI has on our ability to socialize, however. In a <a href="https://www.apa.org/pubs/journals/releases/apl-apl0001103.pdf">series of studies by Pok Man Tang and more</a>, it was found that just replacing coworkers with AI models made people more lonely. The following is the conclusion of the article: 
				</p>
				<sub-block>
					Across millennia, people evolved internal systems to gauge the
					quality of relationships with others. These systems have remained
					effective in a workplace that, just as in primitive tribal communities,
					prioritized social interactions with coworkers. Yet, the advent of
					digital, asocial AI systems and their incorporation into employee
					work, threatens to upend the operation of these systems. <br><br><b>We show
					how employee interactions with their new AI colleagues may lead to
					an increased need for affiliation as well as loneliness.</b><br><br> The mixed
					consequences of these states paint an important, but sobering,
					picture of the future of AI augmentation efforts. While this future
					continues apace, managers must pay heed to outcomes experienced
					by their human employees.
				</sub-block>
				<p>
					Every bit that we eliminate humans from the picture, we're trading efficiency (maybe) for a more and more atomized society. In a world where you work almost exclusively with AI, your therapist is AI, your doctor is AI, you go through the self-checkout lane at the grocery store, and third spaces are further evaporated, and even the internet is filled with bots, what other outcome is there aside from deep existential loneliness?
				</p>
				<h2>
					Anticognition
				</h2>
				<hr>
				<p>
					AI makes us dumber. I get this sounds like any other boomer statement about the "kids these days", but there is a serious case to consider here.
				</p>
				<p>
					Students all around the world are offloading huge portions of their learning to chatbots. It's so much easier to have an AI just write an essay for you than to have to do it yourself. In a project or assignment-heavy class, said assignments lose all of their ability to teach you anything as soon as you employ AI.

					The statistics on this are dismal, with a self-reported <a href="https://www.intelligent.com/4-in-10-college-students-are-using-chatgpt-on-assignments/">37% of college students</a> using AI, with 96% of those students using them for schoolwork.
				</p>
				<p>
					The results of this are pretty much what you would expect. <a href="https://www.sciencedirect.com/science/article/pii/S2451958825000570?via%3Dihub">A study out of the University of Tartu</a> examining how AI usage impacted students learning in an Object Oriented Programming course revealed a generally negative correlation between AI usage and test scores, and a positive correlation with AI usage and perceived homework difficulty.
				</p>
				<sub-block>
					The results show statistically significant relations between frequency of use and academic performance. The use of AI assistants is moderately negatively associated with points scored in programming test 1 (r(229) = -0.315) and weakly negatively associated with all other points. A weak positive correlation was found between the frequency of usage of chatbots and the perceived difficulty of homework to students (r(229) = 0.176). No statistically significant correlations were found between perceived helpfulness and academic performance.
				</sub-block>
				<p>
					There's one important point I want to address both from that study and others like it. Many studies look at the academic performance of students, but this is a poor metric for judging actual learning. It is quite easy to use AI to breeze through lots of assignments while tests are harder to get by.

					Specifically looking at merely "academic performance" is a poor metric.
				</p>
				<h3>The gym of the mind</h3>
				<p>
					To borrow the term from <a href="https://www.youtube.com/watch?v=KPUlgSRn6e0">A pro-urbanism video by Jason Slaughter (NotJustBikes)</a>, the gym of life refers to the fact that how we go about our daily routines impacts our day-to-day health.

					Those that mostly walk or bike instead of driving are healthier by default. Our bodies are meant to be put through some level of strenuous activity, and this is why people in car-centric communities are on average far more unhealthy than those that aren't.
				</p>
				<p>
					So what does this have to do with AI? Just as a sedentary lifestyle is bad for your health, a sedentary mind is bad for your brain. AI encourages the brain equivalent of a sedentary lifestyle.
				</p>
				<p>
					When people use AI tools to help solve their problems, this inherently offloads cognitive work. You have to think less. Using AI once is not going to ruin your mind, but if you
					repeatedly use it over and over again, you're preventing learning.
				</p>
				<p>
					If you think this is bunk, this is the same reason writing notes is more effective for learning than typing them. It forces you to engage more with the text you're writing, meaning it stays in your working memory longer, and is easier to move into long-term memory.
					For things we've already learned, having to go through the process of engaging with a problem again and again maintains and strengthens our existing long-term knowledge. 
				</p>
				<p>
					As soon as AI is introduced in the picture, this entire process is thrown out. You stop learning and you stagnate. Stagnating in your own problem solving abilities, while bad on its own, even has physical health implications and has been linked to the development of dementia.
					It's the same reason people that have learned a second language have lower rates of dementia. Learning a language is <i>hard</i>. Anything difficult you engage with makes your mind slightly healthier. AI is the equivalent of an empty calorie - satiating the problem while offering no nutrition.
				</p>
				<h2>
					The End of Society
				</h2>
				<hr>
				<p>
					This section will admittedly require a lot of speculation. I cannot predict the future. However, I personally think all of my speculation here follows from very basic observations
					and I am very confident in these predictions.
				</p>
				<p>
					AI art is probably the most actively controversial topic in the entire discussion around AI. I will <i>not</i> be arguing whether AI art is art, as I personally feel it's a meaningless war over definitions that addresses none of the real problems this technology brings.
					I am happy to consider it art, as people can and have expressed themselves through all sorts of different mediums. The other thing I will not be arguing about is whether AI is truly "creative" in the same way that a human is.
					
					This is yet another meaningless philosophical distraction with no bearing on reality. My following arguments will apply whether or not you hold that position.
				</p>
				<p>
					Why is AI art bad? Well, firstly I will posit the following: people use art to express themselves. This is the entire point of art. 
				</p>
				<p>
					On the face of it, this isn't really a problem. After all, someone still has to prompt whatever model to generate the art. You may argue that all AI changes about this is that it lowers the barrier for entry and allows more people to easily express their vision. This is a view that fundamentally lacks the vision of the end state of this technology.
				</p>
				<p>
					The final goal is the <i>replacement</i> of all human creation of art. The inevitable consequence is that there will no longer be anyone to prompt the AI, and instead all human expression is going to be replaced by a massive industrial factory that churns out taylor-made art that both responds to and controls the zeitgeist.

					As soon as any human-made work is created that gains any sort of interest, within seconds the mill will detect this trend and push out an immediately created optimized-for-retention slopified version to any potentially interested user. The original work will die, nobody will have any reason to engage with it.
				</p>
				<p>
					We create art to express ourselves, but the underlying truth of it is that we want that art to <i>matter</i>. If nobody would ever care about what you've created, what is the point?
				</p>
				<p>
					This is the scenario that I refer to as the "end of society", with little hyperbole. At this point all of culture is automated, our day to day interactions, interests, and beliefs specifically crafted by a machine that pretends to be us.
				</p>
				<p>
					Obviously not everyone will accept this. There will be plenty of people like myself that value human expression and want to engage with hard work. Public squares, whether virtual or in person, will exist with the express purpose of sharing only human-made media.
					But this doesn't solve the problem. The unavoidable fact is that AI art of all kinds are going to continue getting better, and we've <i>already</i> hit the point where it is often indistinguishable from human made works. Vetting will become nigh-impossible. Our refuges will be instead false islands of security that
					lull us into the accepting of our cultural death.
				</p>
				<p>
					Even ignoring that problem, you still end up with the fact that most people won't care. People like myself are going to be in the minority. Believe it or not, the average person is, if not fine with slop content, at least indifferent enough to it that they don't meaningfully offer pushback. Even if they express otherwise, actions don't lie. It's simply more convenient, and it's designed to be engaging. It's why more people engage with TikTok or Twitter than read in-depth essays. It always has been this way and will never change.
				</p>
				<p>
					Yes, you can generate your perfect vision for your art using Midjourney. But so can anyone else. It will never matter because <i>you</i> did not make it.
				</p>
				<h3>
					The depths beyond the surface
				</h3>
				<p>
					I have really come to value difficulty. When someone has to put in enormous amounts of work, that requires more engagement on their end, meaning there is more to engage with on my end. There is nothing to be examined beyond a surface level in AI generated work.
					AI art advocates always demean photography, but I could never recreate the work of a professional, skilled photographer. In any given work, I could read entire books on the techniques that the photographer used to express their exact vision.

					AI just fundamentally does not offer that.
				</p>
				<p>
					This all may seem a bit handwavy; how exactly is slop AI content worse than slop content that has existed previously? Here's a concrete example - the Five Nights at Freddy's movie. I am taking the position that this was undeniably a corporate slop movie. You may disagree with that statement, as do a lot of my friends, but that isn't the point here. I did not enjoy the movie, but there are things in it that I can respect at least.
					I can clearly tell there was a lot of work put into the movie by many people. Even if the corporate executives didn't care, at least I can tell that some people did. It is made by humans presumably - people had to put together the costumes and plan the setpieces and write the story even if I think the execution was flawed.

					In an AI generated movie, there is no such humanity to respect. It could be "good", it could have a good story and well-polished visuals and high quality audio but why would I care? Respect is two-ways, if there was no respect for the craft put into something there's no reason for me to ever respect it back. 
				</p>
				<p>
					In AI assisted works, perhaps one whose visuals and sound are entirely done with AI but the writing is human made, I only have the writing I can be invested in. The AI generated portion may as well be white noise. Some would say it's only the "vibe of it being made by a person" that matters, but
					this is no different from an argument that could be used by someone that plagiarized another art piece. It's not my fault if I in good faith trusted that your artwork is original when it was actually stolen and thus placed undue respect. I suppose it isn't surprising that the plagiarism analogy is less an analogy and is more or less exactly what's happening. I assure you, we'll get to that!
				</p>
				<p>
					Maybe you don't care. Maybe you take the position that I'm some pretentious asshole that thinks she's better than all the simpletons that enjoy their low-quality content.
				</p>
				<p>
					I don't! I don't care what you want to engage with. Everyone has different tastes and values and I respect that. What I don't respect is that the end goal of this technology is that slop content is all there is going to be.

					You can engage with that. I want something more. I want my words to matter, I want others to know their words matter. I want a world where our endless human passion and creativity has a fucking purpose.
				</p>
				<h3>
					The copyright issue
				</h3>
				<p>
					I cannot even begin to understand how the large corporations behind all of these generative AI platforms haven't been sued into oblivion. Well, I suppose I can. Artists are small and divided up against the titanic industry of generative art.
				</p>
				<p>
					Ever wonder how AI generation in music isn't as prevalent as it is in visual art? It's because musicians are backed by huge record labels that are notoriously eager to destroy anyone that so much as mildly infringes on their intellectual property.
					AI generated music certainly exists, but it's far less of a problem right now than it is in the visual mediums. And this really highlights the problem.
				</p>
				<p>
					<b>Your own creativity is being used to destroy you</b>. The LLM industry could not survive without committing the grandest scale act of copyright infringement in history. You don't have to take this from me, OpenAI even admitted as such:
				</p>
				<sub-block>
					"Because copyright today covers virtually every sort of human expression – including blog posts, photographs, forum posts, scraps of software code, and government documents – it would be impossible to train today's leading AI models without using copyrighted materials. <br> <br>

					Limiting training data to public domain books and drawings created more than a century ago might yield an interesting experiment, but would not provide AI systems that meet the needs of today's citizens.” <br><br>
					<a href="https://www.telegraph.co.uk/business/2024/01/07/openai-warns-copyright-crackdown-could-doom-chatgpt/">OpenAI to the House of Lords communications and digital committee - Reported in The Telegraph, January 7 2024</a>
				</sub-block>
				<p>
					If nothing else happens, I do not believe modern AI companies should be allowed to exist while engaging in this activity. They've gotten away with it because they committed the act fast enough to get big and build an invincible barrier of lawyers before anyone could object. It isn't even possible to opt out - this very article is going to get swallowed up in the endless void of the LLM black hole.
					Were I to have a say, I wouldn't want this article to ever train a model but it's going to get chewed up, mixed with every other piece of writing, and shat back out onto the writing world.
				</p>
				<h2>
					Quick Remarks
				</h2>
				<hr>
				<p>
					There are a few smaller points that need to be addressed that are not significant enough to deserve their own section.
				</p>
				<p>
					<b>This is all capitalism's fault! Not AI's fault!</b>
				</p>
				<p>
					I have bad news for you, whether you're pro-capitalism or anti-capitalism. If you're pro-capitalism, or at the very least believe there is no viable alternative, you have to instead conclude that the problems I have listed are fundamental evils of the technology of LLMs and nothing to do with the system.
				</p>
				<p>
					If you're anti-capitalism, I'm afraid statistically you live in a capitalist society and that's not going to change for the foreseeable future. Should the ghost of Karl Marx himself come back and bring about a communist utopia or something I'm willing to revisit the arguments and weigh my final stance again on the technology.
					I wrote this article to address our current, modern day society, and not some hypothetical future society.
				</p>
				<p>
					<b>But it makes (tedious task) slightly easier!</b>
				</p>
				<p>
					You're right. Maybe we should let this tech rot our brains, destroy our relationships, and drive us further into a surveillance capitalism hellhole just so I don't have to write a cover letter in a job application that nobody is going to read anyways or respond to/read an email that takes a few minutes at most. Maybe it's worth it to create the greatest ever misinformation crisis post-internet so I can save a quick amount of time to google search for the same question.
					Lets let it kill human passion and creativity so I can add a shitty thumbnail to my youtube video/blogpost that nobody wanted anyways.
				</p>
				<p>
					I used to use AI for all of the things people say it makes easier. I genuinely do not believe, for the average person, the benefits of AI are significant enough to justify its myriad of new problems.
				</p>
				<p>
					<b>You're just a luddite that hates new technology!</b>
				</p>
				<p>
					This could not be further from the truth. I am a software developer, and I love learning about techniques and tools I can utilize to improve the work I do. I love reading about new breakthroughs in the sciences and technology. I still follow the development of quantum computers and am excited for if/when they become viable to start tackling real world problems.

					My modern life would not be possible were it not the blood sweat and tears poured into the vast arrays of absolutely fantastical systems that power the world.

					I am even in favor of <i>machine learning</i> as a concept when used well. The advances in protein folding problems brought about by it is a genuine change that will no doubt save untold lives. I am also interested in how it could be used when paired with automatic theorem provers to tackle unsolved math problems.
				</p>
				<p>
					I am not a luddite. I am someone that believes technology should facilitate the wellbeing of the human species, and that the current landscape of the LLM industry is instead a threat to the world just as I believe that leaded gasoline or CFCs are.
				</p>
				<h2>
					Nicotine
				</h2>
				<hr>
				<p>
					We live in an addiction-based society, and LLMs are the newest nicotine. The convenience they offer is addictive, just enough to make us look past the fact that it's killing us.
				</p>
				<p>
					This article is far from exhaustive on all of my problems with LLMs. I deliberately chose to not include topics like the fact that it's enabling surveillance states in a way that could only be conceived by Orwell himself, or the implications of AI in important decisionmaking positions, or how it has made job hunting a nightmare.
					In addition, every single section of this article is at most half of the original topics I wanted to bring up but I cut for clarity.
				</p>
				<p>
					Researching this topic has left me drained in ways I can barely express in words. The more I read, the more I'm convinced of the ruinous nature of this technology.
				</p>
				<p>
					If I must admit something, I used to use LLMs a lot. I engaged with the earliest usage of AI art, I used it to summarize things or write things for me. I used it to supplement my learning and it was making me dumber as a result. I have an addictive personality, making it far too easy for me to slip into relying on it too much. I had to stop myself because I found it was making me a worse problem solver and exacerbating my negative mental experiences.
					After quitting, I expected things to be much harder than they were. As it turns out, I lived on this earth without it far longer than I have lived with it. I am getting along just fine - and in fact far better since leaving it all behind.
				</p>
				<p>
					I encourage you, as with anything, to seriously engage with what this technology is actually offering you. Examine your priors. Does it <i>actually</i> make you productive? Does it <i>actually</i> help you in any way you couldn't get without it? <b>Could you live without it?</b> I understand the benefits this technology can bring, it does automate some tedious tasks that I don't necessarily want to do but ultimately the cost-benefit analysis does not work out. Everyone will be different, but the conclusion I've come to is yes, I can live without it, and will choose to do so because I cannot in good faith engage with this existential technological horror anymore.
				</p>
				<h2>
					A Closing Anecdote
				</h2>
				<hr>
				<p>
					The above was initially how I was going to close this article. However, there was one final incident that took place while writing that so perfectly encapsulates everything, I cannot have it end any other way.
				</p>
				<p>
					I'm currently in my final year of university, and was having to write an essay about some topic related to privacy. The twist was that the professor wanted us to use AI at certain points in the process to aid us, and then reflect on how it improved our writing.
				</p>
				<p>
					To start, after we made an outline, we were supposed to ask an AI of our choice to review our outline and give us advice on how to clean it up. I dropped mine into Google Gemini and it did not give a single piece of advice on the actual structure of the essay, even with prodding.

					All of its "critiques" were instead more advice for when I <i>do</i> write the essay, like how to make each paragraph flow into one another. Maybe helpful, but it wasn't what I asked for.
				</p>
				<p>
					As I was writing the essay, I wanted to find an example of a large organization misusing publicly accessible information that they were allowed to have. However, every query yielded search results related to data breaches, which isn't what I asked for. Even using the exclude tag "-" did not work.

					The single result that wasn't related to data breaches that google listed within the first 20 or so results had nothing to do with data misuse (despite what the *article itself* states in its introduction), and instead went into a list of security incidents at large companies related to internal sabotage.
				</p>
				<p>
					After being thoroughly flabbergasted, I pasted the <i>exact</i> same query into DuckDuckGo and got exactly the type of results I was looking for on the first try, at the top of the page.
				</p>
				<p>
					Finally, when the first draft of the essay was done, I pasted it into Gemini again to get feedback on it as instructed. Most of its critiques were surface level and not all that interesting. Minor wording changes, no suggestions on how to expand on topics (which I knew there were topics that were underexplored, there were parts I was lazy with and did not elaborate very much on). There is one critique that stuck out, however.
				</p>
				<p>
					Its very first, most important critique, was to not "use sources from the future", and to ensure that they were from the present or past. I checked through, all of my sources were from 2025 or earlier. I had, in fact, not received divine prophecy to source my essay.
				</p>
				<p>
					I turned in my essay, blocked Gemini from my computer using Ublock Origin, and will never use an AI tool again.
				</p>
			</main>
		</article>
	</div>
</body>
<script defer src="../js/snowfallbg.js"></script>
<script defer src="../js/code-block-postprocessing.js"></script>
</html>